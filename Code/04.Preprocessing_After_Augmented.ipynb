{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "{\n",
        "  \"nbformat\": 4,\n",
        "  \"nbformat_minor\": 0,\n",
        "  \"metadata\": {\n",
        "    \"kernelspec\": {\n",
        "      \"name\": \"python3\",\n",
        "      \"display_name\": \"Python 3\"\n",
        "    },\n",
        "    \"language_info\": {\n",
        "      \"name\": \"python\"\n",
        "    }\n",
        "  },\n",
        "  \"cells\": [\n",
        "    {\n",
        "      \"cell_type\": \"markdown\",\n",
        "      \"source\": [\n",
        "        \"# Notebook: 04_Preprocessing_After_Augmented_Lokal.ipynb\\n\\n\",\n",
        "        \"## Tujuan\\n\",\n",
        "        \"Notebook ini bertanggung jawab untuk melakukan preprocessing teks akhir pada data yang telah di-augmentasi. Langkah-langkahnya meliputi normalisasi, tokenisasi, penghapusan *stopwords*, *stemming*, dan penghapusan kata yang terlalu sering atau jarang muncul. Hasil akhir dari notebook ini adalah dataset yang siap digunakan untuk *training* model.\"\n",
        "      ]\n",
        "    },\n",
        "    {\n",
        "      \"cell_type\": \"markdown\",\n",
        "      \"source\": [\n",
        "        \"## Pengaturan Awal (Dilakukan di Terminal)\\n\\n\",\n",
        "        \"Pastikan semua *library* yang diperlukan sudah terinstal di lingkungan virtual Anda melalui terminal VS Code.\"\n",
        "      ]\n",
        "    },\n",
        "    {\n",
        "      \"cell_type\": \"code\",\n",
        "      \"source\": [\n",
        "        \"```bash\\n\",\n",
        "        \"# Buka terminal (Ctrl + `) dan jalankan:\\n\",\n",
        "        \"pip install pandas numpy nltk Sastrawi wordcloud seaborn matplotlib openpyxl\\n\",\n",
        "        \"```\"\n",
        "      ],\n",
        "      \"metadata\": {}\n",
        "    },\n",
        "    {\n",
        "      \"cell_type\": \"markdown\",\n",
        "      \"source\": [\n",
        "        \"## 1. Import Library dan Pengaturan Awal\"\n",
        "      ]\n",
        "    },\n",
        "    {\n",
        "      \"cell_type\": \"code\",\n",
        "      \"source\": [\n",
        "        \"import pandas as pd\\n\",\n",
        "        \"import numpy as np\\n\",\n",
        "        \"import re\\n\",\n",
        "        \"import logging\\n\",\n",
        "        \"import nltk\\n\",\n",
        "        \"from nltk.tokenize import word_tokenize\\n\",\n",
        "        \"from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\\n\",\n",
        "        \"from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\\n\",\n",
        "        \"from collections import Counter\\n\",\n",
        "        \"import seaborn as sns\\n\",\n",
        "        \"import matplotlib.pyplot as plt\\n\",\n",
        "        \"from wordcloud import WordCloud\\n\",\n",
        "        \"import os\\n\",\n",
        "        \"\\n\",\n",
        "        \"# Unduh data NLTK jika belum ada\\n\",\n",
        "        \"try:\\n\",\n",
        "        \"    nltk.data.find('tokenizers/punkt')\\n\",\n",
        "        \"except LookupError:\\n\",\n",
        "        \"    nltk.download('punkt')\\n\",\n",
        "        \"    nltk.download('punkt_tab')\\n\",\n",
        "        \"\\n\",\n",
        "        \"# Inisialisasi logging\\n\",\n",
        "        \"logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\\n\",\n",
        "        \"logger = logging.getLogger(__name__)\"\n",
        "      ],\n",
        "      \"metadata\": {}\n",
        "    },\n",
        "    {\n",
        "      \"cell_type\": \"markdown\",\n",
        "      \"source\": [\n",
        "        \"## 2. Memuat Data Hasil Augmentasi\"\n",
        "      ]\n",
        "    },\n",
        "    {\n",
        "      \"cell_type\": \"code\",\n",
        "      \"source\": [\n",
        "        \"try:\\n\",\n",
        "        \"    # Membuat folder output jika belum ada\\n\",\n",
        "        \"    os.makedirs('data', exist_ok=True)\\n\",\n",
        "        \"    os.makedirs('images', exist_ok=True)\\n\",\n",
        "        \"    \\n\",\n",
        "        \"    # Memuat dataset dari folder lokal\\n\",\n",
        "        \"    filename = 'data/gojek_news_augmented.csv'\\n\",\n",
        "        \"    df = pd.read_csv(filename)\\n\",\n",
        "        \"    logger.info(f\\\"Berhasil memuat file '{filename}' dengan {len(df)} baris.\\\")\\n\",\n",
        "        \"except FileNotFoundError:\\n\",\n",
        "        \"    logger.error(f\\\"File '{filename}' tidak ditemukan. Pastikan notebook 03_Augmented telah dijalankan.\\\")\\n\",\n",
        "        \"    df = pd.DataFrame() # Buat DataFrame kosong agar sisa kode tidak error\"\n",
        "      ],\n",
        "      \"metadata\": {}\n",
        "    },\n",
        "    {\n",
        "      \"cell_type\": \"markdown\",\n",
        "      \"source\": [\n",
        "        \"## 3. Definisi Fungsi dan Variabel Preprocessing\"\n",
        "      ]\n",
        "    },\n",
        "    {\n",
        "      \"cell_type\": \"code\",\n",
        "      \"source\": [\n",
        "        \"if not df.empty:\\n\",\n",
        "        \"    # Kata kunci penting yang tidak akan di-stem atau dihapus (jika ada di custom stopwords)\\n\",\n",
        "        \"    important_keywords = ['gojek', 'goto', 'gofood', 'gopay', 'ojol', 'driver', 'umkm']\\n\",\n",
        "        \"    logger.info(f\\\"Kata kunci yang dijaga: {important_keywords}\\\")\\n\",\n",
        "        \"\\n\",\n",
        "        \"    # Fungsi untuk normalisasi teks dasar\\n\",\n",
        "        \"    def normalize_text(text):\\n\",\n",
        "        \"        try:\\n\",\n",
        "        \"            if not isinstance(text, str):\\n\",\n",
        "        \"                return ''\\n\",\n",
        "        \"            text = text.lower()\\n\",\n",
        "        \"            text = re.sub(r'[^\\\\w\\\\s]', ' ', text) # Hapus tanda baca\\n\",\n",
        "        \"            text = re.sub(r'\\\\d+', '', text) # Hapus angka\\n\",\n",
        "        \"            text = re.sub(r'\\\\s+', ' ', text).strip() # Hapus spasi berlebih\\n\",\n",
        "        \"            return text\\n\",\n",
        "        \"        except Exception as e:\\n\",\n",
        "        \"            logger.error(f\\\"Error saat normalisasi: {e}\\\")\\n\",\n",
        "        \"            return text\\n\",\n",
        "        \"\\n\",\n",
        "        \"    # Fungsi untuk menghapus stopwords\\n\",\n",
        "        \"    stopword_factory = StopWordRemoverFactory()\\n\",\n",
        "        \"    stopwords_id = stopword_factory.get_stop_words()\\n\",\n",
        "        \"    custom_stopwords = [\\n\",\n",
        "        \"        'baca', 'juga', 'klik', 'berita', 'cnn', 'indonesia', 'kompas', 'artikel',\\n\",\n",
        "        \"        'scroll', 'lanjut', 'halaman', 'berikut', 'simak', 'penjelasan'\\n\",\n",
        "        \"    ]\\n\",\n",
        "        \"    stopwords_id.extend(custom_stopwords)\\n\",\n",
        "        \"    stopwords_id = list(set(stopwords_id))\\n\",\n",
        "        \"\\n\",\n",
        "        \"    def remove_stopwords(tokens, stopwords_list, preserved_words):\\n\",\n",
        "        \"        return [word for word in tokens if word.lower() not in stopwords_list or word.lower() in preserved_words]\\n\",\n",
        "        \"\\n\",\n",
        "        \"    # Fungsi untuk stemming\\n\",\n",
        "        \"    stemmer_factory = StemmerFactory()\\n\",\n",
        "        \"    stemmer = stemmer_factory.create_stemmer()\\n\",\n",
        "        \"    stem_cache = {}\\n\",\n",
        "        \"\\n\",\n",
        "        \"    def stem_tokens(tokens, preserved_words):\\n\",\n",
        "        \"        stemmed_tokens = []\\n\",\n",
        "        \"        for token in tokens:\\n\",\n",
        "        \"            if token.lower() in preserved_words:\\n\",\n",
        "        \"                stemmed_tokens.append(token)\\n\",\n",
        "        \"            else:\\n\",\n",
        "        \"                if token not in stem_cache:\\n\",\n",
        "        \"                    stem_cache[token] = stemmer.stem(token)\\n\",\n",
        "        \"                stemmed_tokens.append(stem_cache[token])\\n\",\n",
        "        \"        return stemmed_tokens\"\n",
        "      ],\n",
        "      \"metadata\": {}\n",
        "    },\n",
        "    {\n",
        "      \"cell_type\": \"markdown\",\n",
        "      \"source\": [\n",
        "        \"## 4. Menjalankan Pipeline Preprocessing Akhir\"\n",
        "      ]\n",
        "    },\n",
        "    {\n",
        "      \"cell_type\": \"code\",\n",
        "      \"source\": [\n",
        "        \"if not df.empty:\\n\",\n",
        "        \"    # Validasi kolom yang dibutuhkan\\n\",\n",
        "        \"    required_columns = ['judul', 'cleaned_content', 'label', 'is_augmented']\\n\",\n",
        "        \"    if not all(col in df.columns for col in required_columns):\\n\",\n",
        "        \"        raise ValueError(f\\\"DataFrame harus memiliki kolom: {required_columns}\\\")\\n\",\n",
        "        \"\\n\",\n",
        "        \"    # 1. Normalisasi Teks\\n\",\n",
        "        \"    logger.info(\\\"Memulai normalisasi teks...\\\")\\n\",\n",
        "        \"    df['normalized_content'] = df['cleaned_content'].apply(normalize_text)\\n\",\n",
        "        \"\\n\",\n",
        "        \"    # 2. Tokenisasi\\n\",\n",
        "        \"    logger.info(\\\"Memulai tokenisasi...\\\")\\n\",\n",
        "        \"    df['tokens'] = df['normalized_content'].apply(word_tokenize)\\n\",\n",
        "        \"\\n\",\n",
        "        \"    # 3. Stopwords Removal\\n\",\n",
        "        \"    logger.info(f\\\"Memulai penghapusan {len(stopwords_id)} stopwords...\\\")\\n\",\n",
        "        \"    df['tokens_no_stopwords'] = df['tokens'].apply(lambda x: remove_stopwords(x, stopwords_id, important_keywords))\\n\",\n",
        "        \"\\n\",\n",
        "        \"    # 4. Stemming\\n\",\n",
        "        \"    logger.info(\\\"Memulai stemming...\\\")\\n\",\n",
        "        \"    df['tokens_stemmed'] = df['tokens_no_stopwords'].apply(lambda x: stem_tokens(x, important_keywords))\\n\",\n",
        "        \"\\n\",\n",
        "        \"    # Gabungkan token kembali menjadi string untuk langkah selanjutnya\\n\",\n",
        "        \"    df['content_stemmed'] = df['tokens_stemmed'].apply(' '.join)\\n\",\n",
        "        \"\\n\",\n",
        "        \"    # 5. Penghapusan Kata Langka dan Umum\\n\",\n",
        "        \"    logger.info(\\\"Memulai penghapusan kata langka dan umum...\\\")\\n\",\n",
        "        \"    all_words = [word for tokens in df['tokens_stemmed'] for word in tokens]\\n\",\n",
        "        \"    word_frequencies = Counter(all_words)\\n\",\n",
        "        \"    \\n\",\n",
        "        \"    rare_threshold = 2\\n\",\n",
        "        \"    common_threshold = int(df['tokens_stemmed'].apply(len).sum() * 0.005) # 0.5% dari total kata\\n\",\n",
        "        \"\\n\",\n",
        "        \"    rare_words = {word for word, count in word_frequencies.items() if count < rare_threshold}\\n\",\n",
        "        \"    common_words = {word for word, count in word_frequencies.items() if count > common_threshold}\\n\",\n",
        "        \"    \\n\",\n",
        "        \"    # Kecualikan kata kunci penting dari daftar kata yang akan dihapus\\n\",\n",
        "        \"    words_to_remove = (rare_words | common_words) - set(important_keywords)\\n\",\n",
        "        \"    logger.info(f\\\"Akan menghapus {len(words_to_remove)} kata langka/umum.\\\")\\n\",\n",
        "        \"\\n\",\n",
        "        \"    df['tokens_filtered'] = df['tokens_stemmed'].apply(lambda tokens: [word for word in tokens if word not in words_to_remove])\\n\",\n",
        "        \"    df['filtered_content'] = df['tokens_filtered'].apply(' '.join)\\n\",\n",
        "        \"\\n\",\n",
        "        \"    # 6. Validasi Teks Kosong\\n\",\n",
        "        \"    empty_texts_count = (df['filtered_content'].str.strip() == '').sum()\\n\",\n",
        "        \"    if empty_texts_count > 0:\\n\",\n",
        "        \"        logger.warning(f\\\"Terdapat {empty_texts_count} teks kosong setelah preprocessing. Mengisinya kembali dengan konten setelah stemming.\\\")\\n\",\n",
        "        \"        df.loc[df['filtered_content'].str.strip() == '', 'filtered_content'] = df['content_stemmed']\\n\",\n",
        "        \"\\n\",\n",
        "        \"    logger.info(\\\"Pipeline preprocessing akhir selesai.\\\")\\n\",\n",
        "        \"    display(df[['cleaned_content', 'filtered_content', 'label']].head())\"\n",
        "      ],\n",
        "      \"metadata\": {}\n",
        "    },\n",
        "    {\n",
        "      \"cell_type\": \"markdown\",\n",
        "      \"source\": [\n",
        "        \"## 5. Analisis dan Visualisasi Hasil\"\n",
        "      ]\n",
        "    },\n",
        "    {\n",
        "      \"cell_type\": \"code\",\n",
        "      \"source\": [\n",
        "        \"if not df.empty:\\n\",\n",
        "        \"    # Analisis Frekuensi Kata\\n\",\n",
        "        \"    final_words = [word for tokens in df['tokens_filtered'] for word in tokens]\\n\",\n",
        "        \"    freq_df = pd.DataFrame(Counter(final_words).items(), columns=['word', 'count']).sort_values(by='count', ascending=False)\\n\",\n",
        "        \"\\n\",\n",
        "        \"    # Plot Top 50 Kata\\n\",\n",
        "        \"    plt.figure(figsize=(15, 10))\\n\",\n",
        "        \"    sns.barplot(x='count', y='word', data=freq_df.head(50), palette='viridis')\\n\",\n",
        "        \"    plt.title('Top 50 Kata Paling Sering Muncul (Setelah Preprocessing)', fontsize=16)\\n\",\n",
        "        \"    plt.xlabel('Frekuensi', fontsize=14)\\n\",\n",
        "        \"    plt.ylabel('Kata', fontsize=14)\\n\",\n",
        "        \"    plt.tight_layout()\\n\",\n",
        "        \"    plt.savefig('images/gojek_top_50_words_final.png')\\n\",\n",
        "        \"    plt.show()\\n\",\n",
        "        \"\\n\",\n",
        "        \"    # Word Cloud\\n\",\n",
        "        \"    wordcloud_text = ' '.join(df['filtered_content'])\\n\",\n",
        "        \"    if wordcloud_text:\\n\",\n",
        "        \"        wordcloud = WordCloud(width=800, height=400, background_color='white', colormap='viridis').generate(wordcloud_text)\\n\",\n",
        "        \"        plt.figure(figsize=(10, 5))\\n\",\n",
        "        \"        plt.imshow(wordcloud, interpolation='bilinear')\\n\",\n",
        "        \"        plt.axis('off')\\n\",\n",
        "        \"        plt.title('Word Cloud Berita Gojek (Setelah Preprocessing Akhir)', fontsize=16)\\n\",\n",
        "        \"        plt.savefig('images/gojek_wordcloud_final.png')\\n\",\n",
        "        \"        plt.show()\\n\",\n",
        "        \"\\n\",\n",
        "        \"    # Distribusi Label\\n\",\n",
        "        \"    plt.figure(figsize=(8, 6))\\n\",\n",
        "        \"    sns.countplot(x='label', data=df, palette='viridis', hue='label', legend=False)\\n\",\n",
        "        \"    plt.title('Distribusi Label Setelah Augmentasi dan Preprocessing', fontsize=16)\\n\",\n",
        "        \"    plt.xlabel('Label', fontsize=14)\\n\",\n",
        "        \"    plt.ylabel('Jumlah Data', fontsize=14)\\n\",\n",
        "        \"    plt.tight_layout()\\n\",\n",
        "        \"    plt.savefig('images/gojek_label_distribution_final.png')\\n\",\n",
        "        \"    plt.show()\"\n",
        "      ],\n",
        "      \"metadata\": {}\n",
        "    },\n",
        "    {\n",
        "      \"cell_type\": \"markdown\",\n",
        "      \"source\": [\n",
        "        \"## 6. Simpan Hasil Akhir\\n\\n\",\n",
        "        \"Dataset ini adalah output final yang akan menjadi input untuk model klasifikasi sentimen.\"\n",
        "      ]\n",
        "    },\n",
        "    {\n",
        "      \"cell_type\": \"code\",\n",
        "      \"source\": [\n",
        "        \"if not df.empty:\\n\",\n",
        "        \"    # Pilih kolom yang relevan untuk disimpan\\n\",\n",
        "        \"    final_columns = [\\n\",\n",
        "        \"        'judul', 'cleaned_content', 'filtered_content', \\n\",\n",
        "        \"        'label', 'is_augmented', 'published_date', 'source'\\n\",\n",
        "        \"    ]\\n\",\n",
        "        \"    # Filter kolom yang ada di DataFrame untuk menghindari error\\n\",\n",
        "        \"    final_columns_exist = [col for col in final_columns if col in df.columns]\\n\",\n",
        "        \"    \\n\",\n",
        "        \"    output_df = df[final_columns_exist]\\n\",\n",
        "        \"\\n\",\n",
        "        \"    output_file_csv = 'data/gojek_news_preprocessed_final.csv'\\n\",\n",
        "        \"    output_file_excel = 'data/gojek_news_preprocessed_final.xlsx'\\n\",\n",
        "        \"    output_df.to_csv(output_file_csv, index=False)\\n\",\n",
        "        \"    output_df.to_excel(output_file_excel, index=False)\\n\",\n",
        "        \"    logger.info(f\\\"DataFrame final disimpan ke {output_file_csv} dan {output_file_excel}\\\")\\n\",\n",
        "        \"\\n\",\n",
        "        \"    # Tampilkan ringkasan\\n\",\n",
        "        \"    logger.info(f\\\"Total data: {len(output_df)}\\\")\\n\",\n",
        "        \"    logger.info(f\\\"Distribusi label:\\\\n{output_df['label'].value_counts().to_string()}\\\")\\n\",\n",
        "        \"    logger.info(f\\\"Top 10 kata:\\\\n{freq_df.head(10).to_string(index=False)}\\\")\"\n",
        "      ],\n",
        "      \"metadata\": {}\n",
        "    }\n",
        "  ]\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fNZm6T6S59Lo",
        "outputId": "c64c9e13-ad55-4b59-fd68-40fb9252b3b1"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip uninstall nltk -y -q\n",
        "!pip install pandas numpy nltk Sastrawi wordcloud seaborn matplotlib -q\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import logging\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "from collections import Counter\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "# Clear NLTK data and reinstall\n",
        "import shutil\n",
        "import os\n",
        "for path in nltk.data.path:\n",
        "    if os.path.exists(path):\n",
        "        shutil.rmtree(path, ignore_errors=True)\n",
        "\n",
        "# Unduh data NLTK yang diperlukan\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Inisialisasi logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Load data augmentasi\n",
        "try:\n",
        "    df = pd.read_csv('ikn_news_augmented_full_pipeline.csv')\n",
        "    logger.info(\"Berhasil memuat file ikn_news_augmented_full_pipeline.csv\")\n",
        "except FileNotFoundError:\n",
        "    logger.error(\"File ikn_news_augmented_full_pipeline.csv tidak ditemukan.\")\n",
        "    raise\n",
        "\n",
        "# Validasi kolom\n",
        "required_columns = ['judul', 'cleaned_content', 'label', 'is_augmented']\n",
        "if not all(col in df.columns for col in required_columns):\n",
        "    missing_cols = [col for col in required_columns if col not in df.columns]\n",
        "    logger.error(f\"Kolom yang hilang: {missing_cols}\")\n",
        "    raise ValueError(f\"DataFrame tidak memiliki kolom yang diperlukan: {missing_cols}\")\n",
        "\n",
        "# 1. Normalisasi Teks\n",
        "def normalize_text(text):\n",
        "    try:\n",
        "        if not isinstance(text, str):\n",
        "            return ''\n",
        "        text = text.lower()\n",
        "        text = re.sub(r'[^\\w\\s]', ' ', text)\n",
        "        text = re.sub(r'\\d+', '', text)\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error in normalize_text: {e}\")\n",
        "        return text\n",
        "\n",
        "df['normalized_content'] = df['cleaned_content'].apply(normalize_text)\n",
        "logger.info(\"Normalisasi teks selesai.\")\n",
        "\n",
        "# 2. Tokenisasi\n",
        "df['tokens'] = df['normalized_content'].apply(word_tokenize)\n",
        "df['word_count'] = df['tokens'].apply(len)\n",
        "logger.info(\"Tokenisasi dan perhitungan jumlah kata selesai.\")\n",
        "\n",
        "# 3. Stopwords Removal\n",
        "stopword_factory = StopWordRemoverFactory()\n",
        "stopwords_id = stopword_factory.get_stop_words()\n",
        "\n",
        "custom_stopwords = [\n",
        "    'yang', 'dan', 'di', 'dengan', 'untuk', 'pada', 'ini', 'dari', 'ke', 'dalam',\n",
        "    'tersebut', 'adalah', 'oleh', 'sebagai', 'akan', 'telah', 'juga', 'serta', 'atau',\n",
        "    'baca', 'klik', 'berita', 'oikn', 'brwa', 'the', 'isra', 'mikraj', 'rozaq', 'qosdy',\n",
        "    'anggi', 'welem', 'of', 'ustadz', 'abdul', 'darmawi', 'sri', 'elisnawati', 'nurmis',\n",
        "    'syarariyah', 'musmulyadi', 'to', 'scroll', 'jika', 'lebih', 'sampai', 'setelah',\n",
        "    'sebelum', 'antara', 'selama', 'terhadap', 'melalui', 'hingga', 'sekitar', 'sejak'\n",
        "]\n",
        "stopwords_id.extend(custom_stopwords)\n",
        "stopwords_id = list(set(stopwords_id))\n",
        "logger.info(f\"Total {len(stopwords_id)} stopwords digunakan (kecuali 'ikn').\")\n",
        "\n",
        "def remove_stopwords(text, stopwords_list):\n",
        "    try:\n",
        "        if not text:\n",
        "            return ''\n",
        "        words = text.split()\n",
        "        filtered_words = [word for word in words if word not in stopwords_list or word == 'ikn']\n",
        "        return ' '.join(filtered_words)\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error in remove_stopwords: {e}\")\n",
        "        return text\n",
        "\n",
        "df['content_without_stopwords'] = df['normalized_content'].apply(lambda x: remove_stopwords(x, stopwords_id))\n",
        "df['word_count_no_stopwords'] = df['content_without_stopwords'].apply(lambda x: len(x.split()))\n",
        "logger.info(\"Penghapusan stopwords selesai.\")\n",
        "\n",
        "# Lanjutkan dengan langkah-langkah lainnya seperti stemming, dll.\n",
        "# (Bagian kode selanjutnya tetap sama seperti sebelumnya)\n",
        "\n",
        "# 4. Stemming\n",
        "stemmer_factory = StemmerFactory()\n",
        "stemmer = stemmer_factory.create_stemmer()\n",
        "\n",
        "stem_cache = {}\n",
        "\n",
        "def stem_text(text):\n",
        "    try:\n",
        "        if not text:\n",
        "            return ''\n",
        "        if text in stem_cache:\n",
        "            return stem_cache[text]\n",
        "        words = text.split()\n",
        "        stemmed_words = [word if word == 'ikn' else stemmer.stem(word) for word in words]\n",
        "        stemmed_text = ' '.join(stemmed_words)\n",
        "        stem_cache[text] = stemmed_text\n",
        "        return stemmed_text\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error in stem_text: {e}\")\n",
        "        return text\n",
        "\n",
        "df['content_stemmed'] = df['content_without_stopwords'].apply(stem_text)\n",
        "df['word_count_stemmed'] = df['content_stemmed'].apply(lambda x: len(x.split()))\n",
        "logger.info(\"Stemming selesai (kecuali 'ikn').\")\n",
        "\n",
        "# 5. Rare and Common Words Removal\n",
        "def get_word_frequencies(text_series):\n",
        "    all_words = ' '.join(text_series).split()\n",
        "    return Counter(all_words)\n",
        "\n",
        "word_frequencies = get_word_frequencies(df['content_stemmed'])\n",
        "\n",
        "freq_values = list(word_frequencies.values())\n",
        "rare_threshold = 2  # Kata muncul < 2 kali\n",
        "common_threshold = int(np.percentile(freq_values, 99))  # Persentil 99\n",
        "logger.info(f\"Rare threshold: {rare_threshold}, Common threshold: {common_threshold}\")\n",
        "\n",
        "def remove_rare_common_words(text, word_freq, rare_thresh, common_thresh):\n",
        "    if not text:\n",
        "        return ''\n",
        "    words = text.split()\n",
        "    filtered_words = [\n",
        "        word for word in words\n",
        "        if (rare_thresh <= word_freq.get(word, 0) <= common_thresh) or word == 'ikn'\n",
        "    ]\n",
        "    return ' '.join(filtered_words)\n",
        "\n",
        "df['filtered_content'] = df['content_stemmed'].apply(\n",
        "    lambda x: remove_rare_common_words(x, word_frequencies, rare_threshold, common_threshold)\n",
        ")\n",
        "df['word_count_filtered'] = df['filtered_content'].apply(lambda x: len(x.split()))\n",
        "logger.info(\"Penghapusan rare/common words selesai (kecuali 'ikn').\")\n",
        "\n",
        "# 6. Validasi Hasil\n",
        "empty_texts = df[df['filtered_content'].str.strip() == '']\n",
        "if not empty_texts.empty:\n",
        "    logger.warning(f\"Terdapat {len(empty_texts)} teks kosong setelah preprocessing.\")\n",
        "    df.loc[df['filtered_content'].str.strip() == '', 'filtered_content'] = df['normalized_content']\n",
        "\n",
        "# 7. Analisis Frekuensi Kata dan Visualisasi\n",
        "freq_df = get_word_frequencies(df['filtered_content'])\n",
        "freq_df = pd.DataFrame(freq_df.items(), columns=['word', 'count']).sort_values(by='count', ascending=False)\n",
        "\n",
        "# Plot Top 50 Kata\n",
        "plt.figure(figsize=(15, 10))\n",
        "sns.barplot(x='count', y='word', data=freq_df.head(50), palette='viridis')\n",
        "plt.title('Top 50 Most Frequent Words After Preprocessing', fontsize=16)\n",
        "plt.xlabel('Frequency', fontsize=14)\n",
        "plt.ylabel('Words', fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.savefig('top_50_words.png')\n",
        "plt.show()\n",
        "\n",
        "# Word Cloud\n",
        "wordcloud = WordCloud(width=800, height=400, background_color='white', colormap='viridis').generate(\n",
        "    ' '.join(df['filtered_content'])\n",
        ")\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.title('Word Cloud After Preprocessing', fontsize=16)\n",
        "plt.savefig('wordcloud_filtered.png')\n",
        "plt.show()\n",
        "\n",
        "# Distribusi Label\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.countplot(x='label', data=df, palette='viridis')\n",
        "plt.title('Distribusi Label Setelah Preprocessing', fontsize=16)\n",
        "plt.xlabel('Label', fontsize=14)\n",
        "plt.ylabel('Jumlah Data', fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.savefig('label_distribution.png')\n",
        "plt.show()\n",
        "\n",
        "# 8. Simpan Hasil\n",
        "output_file_csv = 'ikn_news_preprocessed_final.csv'\n",
        "output_file_excel = 'ikn_news_preprocessed_final.xlsx'\n",
        "df.to_csv(output_file_csv, index=False)\n",
        "df.to_excel(output_file_excel, index=False)\n",
        "logger.info(f\"DataFrame disimpan ke {output_file_csv} dan {output_file_excel}\")\n",
        "\n",
        "# Ringkasan\n",
        "logger.info(f\"Total data: {len(df)}\")\n",
        "logger.info(f\"Distribusi label:\\n{df['label'].value_counts().to_string()}\")\n",
        "logger.info(f\"Top 10 kata:\\n{freq_df.head(10).to_string()}\")\n",
        "logger.info(df[['judul', 'filtered_content', 'label', 'is_augmented']].head().to_string())"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
