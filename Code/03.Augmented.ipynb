{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Notebook: 03_Augmentasi_Data_Lokal.ipynb\n",
        "\n",
        "## Pengaturan Awal (Dilakukan di Terminal)\n",
        "\n",
        "Jalankan instalasi dependensi di terminal VS Code (bukan di dalam notebook):\n",
        "\n",
        "```bash\n",
        "pip install pandas deep-translator langdetect \"nltk==3.8.1\" Sastrawi retry openpyxl\n",
        "```\n",
        "\n",
        "Setelah instalasi selesai, lanjutkan menjalankan sel-sel di bawah ini.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 1: Import library dan setup logging\n",
        "import pandas as pd\n",
        "import re\n",
        "import logging\n",
        "from deep_translator import GoogleTranslator\n",
        "from langdetect import detect, LangDetectException\n",
        "import random\n",
        "import time\n",
        "from retry import retry\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "from multiprocessing import Pool, cpu_count\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Cell 2: Unduh paket NLTK (cukup dijalankan sekali)\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    nltk.download('punkt')\n",
        "    nltk.download('punkt_tab')\n",
        "print(\"Paket NLTK sudah siap.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 3: Utilitas dan kamus\n",
        "\n",
        "def truncate_text(text, max_length=4500):\n",
        "    if len(text) > max_length:\n",
        "        return text[:max_length]\n",
        "    return text\n",
        "\n",
        "translation_cache = {}\n",
        "\n",
        "@retry(tries=3, delay=2, backoff=2)\n",
        "def back_translate(text, intermediate_lang='en'):\n",
        "    try:\n",
        "        text = truncate_text(text)\n",
        "        if text in translation_cache:\n",
        "            return translation_cache[text]\n",
        "        translated = GoogleTranslator(source='id', target=intermediate_lang).translate(text)\n",
        "        time.sleep(0.5)\n",
        "        back_translated = GoogleTranslator(source=intermediate_lang, target='id').translate(translated)\n",
        "        time.sleep(0.5)\n",
        "        if back_translated:\n",
        "            translation_cache[text] = back_translated\n",
        "            return back_translated\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error pada back-translation: {e}\")\n",
        "        return text\n",
        "\n",
        "synonym_dict = {\n",
        "    'tidak': ['bukan', 'tak', 'jangan'],\n",
        "    'pembangunan': ['konstruksi', 'pengembangan', 'pendirian'],\n",
        "    'proyek': ['rencana', 'pekerjaan', 'program'],\n",
        "    'investasi': ['penanaman modal', 'pendanaan', 'modal'],\n",
        "    'infrastruktur': ['sarana', 'prasarana', 'fasilitas'],\n",
        "    'layanan': ['jasa', 'servis'],\n",
        "    'driver': ['pengemudi', 'mitra'],\n",
        "    'tarif': ['ongkos', 'biaya'],\n",
        "    'aplikasi': ['platform', 'sistem']\n",
        "}\n",
        "\n",
        "gojek_keywords = [\n",
        "    'gojek', 'goto', 'ojol', 'driver', 'aplikasi', 'layanan', 'tarif',\n",
        "    'gofood', 'gopay', 'transportasi', 'ojek', 'online'\n",
        "]\n",
        "\n",
        "stemmer = StemmerFactory().create_stemmer()\n",
        "\n",
        "\n",
        "def synonym_replacement(text, n=3):\n",
        "    try:\n",
        "        words = word_tokenize(text)\n",
        "        new_words = words.copy()\n",
        "        indices = list(range(len(words)))\n",
        "        random.shuffle(indices)\n",
        "        replaced = 0\n",
        "        for i in indices:\n",
        "            word = words[i].lower()\n",
        "            if word in synonym_dict:\n",
        "                synonyms = [syn for syn in synonym_dict[word] if syn != word]\n",
        "                if synonyms:\n",
        "                    new_words[i] = random.choice(synonyms)\n",
        "                    replaced += 1\n",
        "            if replaced >= n:\n",
        "                break\n",
        "        return \" \".join(new_words)\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error in synonym_replacement: {e}\")\n",
        "        return text\n",
        "\n",
        "\n",
        "def simple_paraphrase(text):\n",
        "    try:\n",
        "        words = word_tokenize(text)\n",
        "        if len(words) < 5:\n",
        "            return text\n",
        "        key_positions = [i for i, w in enumerate(words) if w.lower() in gojek_keywords]\n",
        "        if len(key_positions) < 2:\n",
        "            return text\n",
        "        pos1, pos2 = random.sample(key_positions, 2)\n",
        "        words[pos1], words[pos2] = words[pos2], words[pos1]\n",
        "        return \" \".join(words)\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error in simple_paraphrase: {e}\")\n",
        "        return text\n",
        "\n",
        "\n",
        "def validate_augmented_text(text):\n",
        "    try:\n",
        "        if len(text.split()) < 5:\n",
        "            return False\n",
        "        try:\n",
        "            is_id = detect(text) == 'id'\n",
        "        except LangDetectException:\n",
        "            is_id = False\n",
        "        if not is_id:\n",
        "            return False\n",
        "        keyword_count = sum(1 for keyword in gojek_keywords if keyword.lower() in text.lower())\n",
        "        return keyword_count >= 2\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error in validate_augmented_text: {e}\")\n",
        "        return False\n",
        "\n",
        "\n",
        "def clean_augmented_text(text):\n",
        "    try:\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "        text = re.sub(r'[^\\w\\s.,!?éê]', '', text)\n",
        "        words = word_tokenize(text)\n",
        "        stemmed_words = [stemmer.stem(w) if w.lower() not in gojek_keywords else w for w in words]\n",
        "        return \" \".join(stemmed_words)\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error in clean_augmented_text: {e}\")\n",
        "        return text\n",
        "\n",
        "\n",
        "def augment_row(row, intermediate_langs):\n",
        "    try:\n",
        "        original_text = row['cleaned_content']\n",
        "        method = random.choice(['back_translate', 'synonym', 'paraphrase'])\n",
        "        if method == 'back_translate' and len(original_text) > 50:\n",
        "            intermediate_lang = random.choice(intermediate_langs)\n",
        "            augmented_text = back_translate(original_text, intermediate_lang=intermediate_lang)\n",
        "        elif method == 'synonym':\n",
        "            augmented_text = synonym_replacement(original_text, n=3)\n",
        "        else:\n",
        "            augmented_text = simple_paraphrase(original_text)\n",
        "\n",
        "        if augmented_text and augmented_text != original_text:\n",
        "            cleaned_aug_text = clean_augmented_text(augmented_text)\n",
        "            if validate_augmented_text(cleaned_aug_text):\n",
        "                new_row = row.copy()\n",
        "                new_row['cleaned_content'] = cleaned_aug_text\n",
        "                new_row['is_augmented'] = True\n",
        "                return new_row\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error dalam augment_row: {e}\")\n",
        "        return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 4: Pipeline Augmentasi Utama\n",
        "if __name__ == '__main__':\n",
        "    # Muat file hasil preprocessing\n",
        "    try:\n",
        "        df = pd.read_csv('data/gojek_news_preprocessed.csv')\n",
        "        logger.info(f\"Berhasil memuat file dengan {len(df)} baris.\")\n",
        "    except FileNotFoundError:\n",
        "        logger.error(\"File 'data/gojek_news_preprocessed.csv' tidak ditemukan. Jalankan notebook preprocessing terlebih dahulu.\")\n",
        "        raise SystemExit(1)\n",
        "\n",
        "    # Pastikan kolom label ada\n",
        "    if 'label' not in df.columns:\n",
        "        logger.warning(\"Kolom 'label' tidak ditemukan. Membuat kolom label dummy [positif, netral, negatif].\")\n",
        "        df['label'] = np.random.choice(['positif', 'netral', 'negatif'], size=len(df), p=[0.2, 0.6, 0.2])\n",
        "\n",
        "    TARGET_PER_LABEL = 150\n",
        "    INTERMEDIATE_LANGS = ['en', 'fr', 'de', 'es']\n",
        "\n",
        "    final_dfs = []\n",
        "    logger.info(f\"Memulai proses augmentasi untuk mencapai target {TARGET_PER_LABEL} data per label...\")\n",
        "\n",
        "    for label in df['label'].unique():\n",
        "        subset = df[df['label'] == label].copy()\n",
        "        count = len(subset)\n",
        "        logger.info(f\"Label '{label}' memiliki {count} data.\")\n",
        "\n",
        "        if count >= TARGET_PER_LABEL:\n",
        "            sampled = subset.sample(n=TARGET_PER_LABEL, random_state=42).copy()\n",
        "            sampled['is_augmented'] = False\n",
        "            final_dfs.append(sampled)\n",
        "        else:\n",
        "            subset['is_augmented'] = False\n",
        "            final_dfs.append(subset)\n",
        "            n_to_augment = TARGET_PER_LABEL - count\n",
        "            logger.info(f\"Label '{label}': Perlu augmentasi sebanyak {n_to_augment} data.\")\n",
        "\n",
        "            # Oversample untuk antisipasi kegagalan validasi\n",
        "            rows_to_augment = [subset.iloc[i % count].copy() for i in range(n_to_augment * 2)]\n",
        "\n",
        "            with Pool(processes=cpu_count()) as pool:\n",
        "                results = pool.starmap(augment_row, [(row, INTERMEDIATE_LANGS) for row in rows_to_augment])\n",
        "\n",
        "            augmented_rows = [row for row in results if row is not None][:n_to_augment]\n",
        "\n",
        "            if len(augmented_rows) < n_to_augment:\n",
        "                logger.warning(f\"Label '{label}': Hanya {len(augmented_rows)} data berhasil diaugmentasi. Sisanya diisi dengan duplikasi.\")\n",
        "                remaining = n_to_augment - len(augmented_rows)\n",
        "                fallback_rows = [subset.iloc[i % count].copy() for i in range(remaining)]\n",
        "                for row in fallback_rows:\n",
        "                    row['is_augmented'] = False\n",
        "                augmented_rows.extend(fallback_rows)\n",
        "\n",
        "            df_aug = pd.DataFrame(augmented_rows)\n",
        "            final_dfs.append(df_aug)\n",
        "\n",
        "    df_final = pd.concat(final_dfs, ignore_index=True)\n",
        "\n",
        "    output_path_csv = 'data/gojek_news_augmented.csv'\n",
        "    output_path_xlsx = 'data/gojek_news_augmented.xlsx'\n",
        "    df_final.to_csv(output_path_csv, index=False)\n",
        "    df_final.to_excel(output_path_xlsx, index=False)\n",
        "\n",
        "    logger.info(\"--- PROSES SELESAI ---\")\n",
        "    logger.info(f\"Data augmented disimpan di '{output_path_csv}' dan '{output_path_xlsx}'\")\n",
        "    logger.info(f\"Total data akhir: {len(df_final)}\")\n",
        "    logger.info(f\"Distribusi label baru:\\n{df_final['label'].value_counts().to_string()}\")\n",
        "\n",
        "    print(\"\\nSampel data hasil augmentasi:\")\n",
        "    try:\n",
        "        display(df_final.sample(10)[['cleaned_content', 'label', 'is_augmented']])\n",
        "    except Exception:\n",
        "        print(df_final.sample(10)[['cleaned_content', 'label', 'is_augmented']])\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
