{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8CMDW93u-0WK",
        "outputId": "d76473d6-a117-4c34-bcf2-1955757364d1"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip uninstall nltk -y -q\n",
        "!pip install pandas numpy nltk Sastrawi wordcloud seaborn matplotlib scikit-learn -q\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import logging\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "from collections import Counter\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Clear NLTK data and reinstall\n",
        "import shutil\n",
        "import os\n",
        "for path in nltk.data.path:\n",
        "    if os.path.exists(path):\n",
        "        shutil.rmtree(path, ignore_errors=True)\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Inisialisasi logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Load data preprocessed\n",
        "try:\n",
        "    df = pd.read_csv('ikn_news_preprocessed_final.csv')  # Perbaikan: Ganti file dan tambah tanda kutip penutup\n",
        "    logger.info(\"Berhasil memuat file ikn_news_preprocessed_final.csv\")\n",
        "except FileNotFoundError:\n",
        "    logger.error(\"ikn_news_preprocessed_final.csv tidak ditemukan.\")\n",
        "    raise\n",
        "\n",
        "# Pastikan kolom yang diperlukan ada\n",
        "required_columns = ['filtered_content', 'label']  # Gunakan filtered_content yang sudah dipreproses\n",
        "if not all(col in df.columns for col in required_columns):\n",
        "    missing_cols = [col for col in required_columns if col not in df.columns]\n",
        "    logger.error(f\"Kolom yang hilang: {missing_cols}\")\n",
        "    raise ValueError(f\"DataFrame tidak memiliki kolom yang diperlukan: {missing_cols}\")\n",
        "\n",
        "# Pisahkan fitur (teks) dan label\n",
        "X = df['filtered_content'].fillna('')  # Gunakan filtered_content sebagai input\n",
        "y = df['label']\n",
        "\n",
        "# Bagi data menjadi train dan test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Definisikan pipeline dengan TF-IDF dan MultinomialNB\n",
        "pipeline_nb = make_pipeline(\n",
        "    TfidfVectorizer(\n",
        "        stop_words=['yang', 'dan', 'di', 'dengan', 'untuk', 'pada', 'ini', 'dari', 'ke', 'dalam', 'tersebut', 'adalah', 'oleh'],\n",
        "        max_features=5000,\n",
        "        ngram_range=(1, 2)  # Tambahkan bigram untuk konteks lebih baik\n",
        "    ),\n",
        "    MultinomialNB()\n",
        ")\n",
        "\n",
        "# Definisikan parameter grid untuk tuning\n",
        "param_grid = {\n",
        "    'tfidfvectorizer__max_features': [3000, 5000, 7000],\n",
        "    'multinomialnb__alpha': [0.1, 0.5, 1.0]\n",
        "}\n",
        "\n",
        "# Lakukan GridSearchCV untuk menemukan parameter terbaik\n",
        "grid_search = GridSearchCV(pipeline_nb, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Cetak parameter terbaik\n",
        "print(f\"Parameter terbaik: {grid_search.best_params_}\")\n",
        "print(f\"Skor terbaik: {grid_search.best_score_:.2f}\")\n",
        "\n",
        "# Evaluasi model terbaik\n",
        "y_pred = grid_search.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"\\nAkurasi Model pada Test Set: {accuracy:.2f}\")\n",
        "print(\"\\nLaporan Klasifikasi:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Prediksi teks baru\n",
        "new_texts = [\n",
        "    \"Proyek IKN berkembang pesat dengan dukungan pemerintah\",\n",
        "    \"Kontroversi kebijakan IKN memicu debat publik\"\n",
        "]\n",
        "predictions = grid_search.predict(new_texts)\n",
        "\n",
        "# Menampilkan hasil prediksi\n",
        "for text, pred in zip(new_texts, predictions):\n",
        "    print(f\"\\nTeks: '{text}'\")\n",
        "    print(f\"Kategori Prediksi: {pred}\")\n",
        "\n",
        "# Visualisasi distribusi label\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.countplot(x='label', data=df, palette='viridis')\n",
        "plt.title('Distribusi Label', fontsize=16)\n",
        "plt.xlabel('Label', fontsize=14)\n",
        "plt.ylabel('Jumlah Data', fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.savefig('label_distribution.png')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Buka terminal (Ctrl + `) dan jalankan:\n",
        "pip install pandas numpy scikit-learn seaborn matplotlib joblib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# In [ ]:\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import logging\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import joblib\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Inisialisasi logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Buat folder yang diperlukan jika belum ada\n",
        "os.makedirs('data', exist_ok=True)\n",
        "os.makedirs('images', exist_ok=True)\n",
        "os.makedirs('models', exist_ok=True)\n",
        "\n",
        "# Load data hasil preprocessing akhir (dari notebook 04)\n",
        "try:\n",
        "    df = pd.read_csv('data/gojek_news_preprocessed_final.csv')\n",
        "    logger.info(\"Berhasil memuat file 'gojek_news_preprocessed_final.csv'\")\n",
        "except FileNotFoundError:\n",
        "    logger.error(\"'gojek_news_preprocessed_final.csv' tidak ditemukan. Pastikan notebook 04 sudah dijalankan.\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# In [ ]:\n",
        "# Pastikan kolom yang diperlukan ada\n",
        "required_columns = ['filtered_content', 'label']\n",
        "if not all(col in df.columns for col in required_columns):\n",
        "    raise ValueError(f\"DataFrame tidak memiliki kolom yang diperlukan: {required_columns}\")\n",
        "\n",
        "# Pisahkan fitur (X) dan label (y)\n",
        "X = df['filtered_content'].fillna('')  # Mengisi nilai NaN dengan string kosong\n",
        "y = df['label']\n",
        "\n",
        "# Bagi data menjadi data latih (train) dan data uji (test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "logger.info(f\"Ukuran data latih: {len(X_train)} sampel\")\n",
        "logger.info(f\"Ukuran data uji: {len(X_test)} sampel\")\n",
        "\n",
        "# Definisikan pipeline model\n",
        "pipeline_nb = make_pipeline(\n",
        "    TfidfVectorizer(\n",
        "        ngram_range=(1, 2)  # Menggunakan unigram dan bigram\n",
        "    ),\n",
        "    MultinomialNB()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# In [ ]:\n",
        "# Definisikan parameter grid untuk diuji\n",
        "param_grid = {\n",
        "    'tfidfvectorizer__max_features': [3000, 5000, 7000],\n",
        "    'tfidfvectorizer__max_df': [0.75, 0.9, 1.0],\n",
        "    'multinomialnb__alpha': [0.1, 0.5, 1.0]\n",
        "}\n",
        "\n",
        "# Lakukan pencarian parameter terbaik menggunakan cross-validation\n",
        "logger.info(\"Memulai GridSearchCV untuk mencari parameter terbaik...\")\n",
        "grid_search = GridSearchCV(pipeline_nb, param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Tampilkan hasil terbaik\n",
        "print(\"\\n--- Hasil GridSearchCV ---\")\n",
        "print(f\"Parameter terbaik: {grid_search.best_params_}\")\n",
        "print(f\"Skor akurasi cross-validation terbaik: {grid_search.best_score_:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# In [ ]:\n",
        "# Gunakan model terbaik untuk prediksi pada data uji\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "report = classification_report(y_test, y_pred, digits=3)\n",
        "\n",
        "print(f\"\\nAkurasi Model pada Data Uji: {accuracy:.3f}\")\n",
        "print(\"\\nLaporan Klasifikasi:\\n\")\n",
        "print(report)\n",
        "\n",
        "# Visualisasi Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred, labels=best_model.classes_)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "            xticklabels=best_model.classes_, yticklabels=best_model.classes_)\n",
        "plt.title('Confusion Matrix - TF-IDF + MultinomialNB (Gojek)', fontsize=15)\n",
        "plt.xlabel('Prediksi', fontsize=12)\n",
        "plt.ylabel('Aktual', fontsize=12)\n",
        "plt.savefig('images/gojek_tfidf_confusion_matrix.png')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# In [ ]:\n",
        "# Contoh kalimat baru yang relevan dengan Gojek\n",
        "new_texts = [\n",
        "    \"Layanan gojek sangat membantu mobilitas saya sehari-hari, cepat dan efisien.\",\n",
        "    \"Tarif ojol sekarang terasa terlalu mahal dan sering kali orderan dibatalkan oleh driver.\",\n",
        "    \"Gojek mengumumkan akan memperluas jangkauan operasional ke beberapa kota baru.\"\n",
        "]\n",
        "\n",
        "predictions = best_model.predict(new_texts)\n",
        "prediction_proba = best_model.predict_proba(new_texts)\n",
        "\n",
        "print(\"--- Hasil Prediksi pada Kalimat Baru ---\\n\")\n",
        "for text, pred, proba in zip(new_texts, predictions, prediction_proba):\n",
        "    print(f\"Teks: '{text}'\")\n",
        "    print(f\"Prediksi Sentimen: {pred}\")\n",
        "    # Tampilkan probabilitas untuk setiap kelas\n",
        "    proba_dict = {label: f\"{p:.2%}\" for label, p in zip(best_model.classes_, proba)}\n",
        "    print(f\"Probabilitas: {proba_dict}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# In [ ]:\n",
        "model_path = 'models/tfidf_nb_model.joblib'\n",
        "joblib.dump(best_model, model_path)\n",
        "logger.info(f\"Model terbaik telah disimpan di: {model_path}\")\n",
        "\n",
        "# Contoh cara memuat model kembali:\n",
        "# loaded_model = joblib.load(model_path)\n",
        "# print(\"Model berhasil dimuat kembali.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
