{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Notebook: 02_Preprocessing_Before_Augmented_Lokal.ipynb\n",
        "\n",
        "## Pengaturan Awal (Dilakukan di Terminal)\n",
        "\n",
        "Sebelum menjalankan kode di notebook, pastikan Anda menginstal semua library yang diperlukan melalui terminal VS Code (dengan lingkungan virtual yang sudah aktif). Ini menggantikan sel `!pip install`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "```bash\n",
        "# Buka terminal di VS Code (Ctrl + `) dan jalankan:\n",
        "pip install pandas emoji langdetect scikit-learn regex matplotlib seaborn wordcloud openpyxl\n",
        "```\n",
        "\n",
        "*(Catatan: `openpyxl` ditambahkan agar bisa menyimpan file ke format `.xlsx`)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import Library & Pengaturan Awal\n",
        "\n",
        "Di sel pertama notebook, kita akan mengimpor semua library dan melakukan pengaturan dasar seperti logging dan membuat folder output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Memuat Dataset dari File Lokal\n",
        "\n",
        "Bagian ini adalah pengganti utama dari `files.upload()`. Kita akan memuat file CSV langsung dari folder `data` di proyek Anda."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aCGVzNBrj16I",
        "outputId": "bf89e228-dfd2-42d0-fe66-8cdd547872a7"
      },
      "outputs": [],
      "source": [
        "# Cell 1: Import Library\n",
        "import pandas as pd\n",
        "import re\n",
        "import emoji\n",
        "import string\n",
        "import logging\n",
        "import os\n",
        "from langdetect import detect, LangDetectException\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import matplotlib.dates as dates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Tklbazaj3wD"
      },
      "outputs": [],
      "source": [
        "# Cell 2: Inisialisasi Logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "52ajVQn9j5gm",
        "outputId": "ab104c8f-4113-4a55-ea6c-ce134db40865"
      },
      "outputs": [],
      "source": [
        "# Cell 3: Buat folder untuk menyimpan output jika belum ada\n",
        "# Ini adalah praktik yang baik untuk menjaga kerapian proyek\n",
        "os.makedirs('data', exist_ok=True)\n",
        "os.makedirs('images', exist_ok=True)\n",
        "logger.info(\"Folder 'data' dan 'images' siap digunakan.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 4: Memuat dataset dari folder lokal\n",
        "try:\n",
        "    # Ganti 'Hasil_Scraping_Gojek.csv' dengan nama file hasil scraping Anda\n",
        "    filename = 'data/Hasil_Scraping_Gojek.csv' \n",
        "    df = pd.read_csv(filename)\n",
        "    logger.info(f\"Dataset {filename} dimuat dengan {len(df)} baris.\")\n",
        "except FileNotFoundError:\n",
        "    logger.error(f\"File '{filename}' tidak ditemukan. Pastikan file tersebut ada di dalam folder 'data'.\")\n",
        "    df = pd.DataFrame() # Buat DataFrame kosong agar sisa kode tidak error\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eU6EWHvEj7GO"
      },
      "outputs": [],
      "source": [
        "# Cell 5: Validasi kolom awal\n",
        "if not df.empty:\n",
        "    required_columns = ['judul', 'content', 'published_date']\n",
        "    # Sesuaikan 'tanggal' menjadi 'published_date' agar cocok dengan output scraper sebelumnya\n",
        "    if not all(col in df.columns for col in required_columns):\n",
        "        missing_cols = [col for col in required_columns if col not in df.columns]\n",
        "        logger.error(f\"Kolom yang hilang: {missing_cols}\")\n",
        "        raise ValueError(f\"DataFrame harus memiliki kolom: {required_columns}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iFnYP07uj8vh"
      },
      "outputs": [],
      "source": [
        "# Cell 6: Konfigurasi variabel dan pola\n",
        "if not df.empty:\n",
        "    initial_count = len(df)\n",
        "    # Sesuaikan akronim dan kata kunci ini dengan topik Gojek Anda\n",
        "    important_acronyms = ['GOTO', 'UMKM', 'Gojek', 'GoFood', 'GoPay']\n",
        "    gojek_keywords = ['gojek', 'goto', 'ojol', 'driver', 'aplikasi', 'layanan', 'tarif', 'gofood', 'gopay', 'transportasi']\n",
        "    \n",
        "    # Kamus normalisasi bisa Anda sesuaikan atau tambahkan\n",
        "    normalization_dict = {\n",
        "        'ga': 'tidak', 'nggak': 'tidak', 'gak': 'tidak', 'tdk': 'tidak',\n",
        "        'yg': 'yang', 'dgn': 'dengan', 'utk': 'untuk', 'dr': 'dari',\n",
        "        'go-jek': 'Gojek', 'go-food': 'GoFood', 'go-pay': 'GoPay'\n",
        "    }\n",
        "    \n",
        "    # Pola Regex untuk membersihkan teks boilerplate dari berita (bisa ditambahkan)\n",
        "    patterns_to_remove = [\n",
        "        r'baca juga:.*', r'lihat juga:.*', r'artikel ini telah tayang.*',\n",
        "        r'jakarta, cnn indonesia —.*', r'jakarta, kompas.com -.*'\n",
        "    ]"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Definisi Fungsi Preprocessing\n",
        "\n",
        "Fungsi-fungsi ini adalah inti dari proses pembersihan data. Logikanya kita pertahankan sepenuhnya, hanya nama variabel `ikn_keywords` yang disesuaikan menjadi `gojek_keywords`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f2VqkX9kj_it"
      },
      "outputs": [],
      "source": [
        "# Fungsi preprocessing terintegrasi\n",
        "def preprocess_text(text):\n",
        "    try:\n",
        "        # Standarisasi akronim\n",
        "        for acronym in important_acronyms:\n",
        "            text = re.sub(rf'(?i)\\b{acronym}\\b', acronym, str(text))\n",
        "\n",
        "        # Hapus emoji dan karakter khusus\n",
        "        text = emoji.replace_emoji(text, replace='')\n",
        "        text = re.sub(r'[^\\w\\s.,!?éê]', '', text)\n",
        "\n",
        "        # Normalisasi teks\n",
        "        for slang, formal in normalization_dict.items():\n",
        "            text = re.sub(rf'\\b{slang}\\b', formal, text, flags=re.IGNORECASE)\n",
        "        text = re.sub(r',(?!\\s)', ', ', text)\n",
        "        text = text.replace('\\\\t', ' ').replace('\\\\n', ' ').replace('\\\\u', ' ').replace('\\\\', '')\n",
        "        text = re.sub(r'(?i)(?:https?:\\/\\/)?(?:www\\.)?(?:[a-zA-Z0-9-.]+)(?:\\.[a-zA-Z]{2,6})(?:\\/[^\\s\\r\\n]*)?|(bit\\.ly|tinyurl\\.com|t\\.co)\\/[^\\s\\r\\n]*', '', text)\n",
        "        text = re.sub(r'\\b\\d{1,3}\\b(?!\\s*(?:miliar|triliun|hektar|tahun))', '', text)\n",
        "        text = text.translate(str.maketrans('', '', string.punctuation.replace(',', '').replace('.', '')))\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "        # Hapus teks tidak relevan\n",
        "        for pattern in patterns_to_remove:\n",
        "            text = re.sub(pattern, '', text)\n",
        "        for text_to_remove in texts_to_remove:\n",
        "            text = text.replace(text_to_remove, '')\n",
        "\n",
        "        # Lowercasing dengan pengecualian\n",
        "        for acronym in important_acronyms:\n",
        "            text = re.sub(rf'\\b{acronym}\\b', f'__{acronym}__', text, flags=re.IGNORECASE)\n",
        "        text = text.lower()\n",
        "        for acronym in important_acronyms:\n",
        "            text = text.replace(f'__{acronym}__', acronym)\n",
        "\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error in preprocess_text: {e}\")\n",
        "        return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RYBN4IhWkEQe"
      },
      "outputs": [],
      "source": [
        "# Cell 8: Fungsi deteksi bahasa dan relevansi\n",
        "def is_valid_text(text):\n",
        "    try:\n",
        "        is_id = detect(text) == 'id'\n",
        "        keyword_count = sum(1 for keyword in gojek_keywords if keyword.lower() in text.lower())\n",
        "        return is_id and keyword_count >= 2\n",
        "    except LangDetectException:\n",
        "        keyword_count = sum(1 for keyword in gojek_keywords if keyword.lower() in text.lower())\n",
        "        return keyword_count >= 2\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error in is_valid_text: {e}\")\n",
        "        return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d3GYhfr5kFnK"
      },
      "outputs": [],
      "source": [
        "# Cell 9: Fungsi hapus duplikasi berdasarkan konten\n",
        "def remove_duplicates(df_to_clean):\n",
        "    try:\n",
        "        vectorizer = TfidfVectorizer(max_features=5000)\n",
        "        tfidf_matrix = vectorizer.fit_transform(df_to_clean['cleaned_content'].fillna(''))\n",
        "        cosine_sim = cosine_similarity(tfidf_matrix)\n",
        "        indices_to_remove = set()\n",
        "        for i in range(len(cosine_sim)):\n",
        "            for j in range(i + 1, len(cosine_sim)):\n",
        "                if cosine_sim[i][j] > 0.9: # Threshold kesamaan 90%\n",
        "                    indices_to_remove.add(j)\n",
        "        return df_to_clean.drop(index=list(indices_to_remove)).reset_index(drop=True)\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error in remove_duplicates: {e}\")\n",
        "        return df_to_clean"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Menjalankan Pipeline Preprocessing\n",
        "\n",
        "Di sini kita akan menjalankan semua fungsi pada DataFrame. Kodenya sedikit disesuaikan untuk menghindari `SettingWithCopyWarning`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SAjAOc4KkHur",
        "outputId": "03e8e6db-aea2-4ea4-e45f-0cd4b6c93a4a"
      },
      "outputs": [],
      "source": [
        "# Cell 10: Proses preprocessing utama\n",
        "if not df.empty:\n",
        "    logger.info(\"Memulai preprocessing...\")\n",
        "    # Ganti 'tanggal' menjadi 'published_date'\n",
        "    df['published_date'] = pd.to_datetime(df['published_date'], errors='coerce')\n",
        "    df = df.dropna(subset=['content']).copy()\n",
        "\n",
        "    logger.info(\"Menerapkan pembersihan pada teks...\")\n",
        "    df['cleaned_content'] = df['content'].apply(preprocess_text)\n",
        "    df['cleaned_title'] = df['judul'].apply(preprocess_text)\n",
        "\n",
        "    logger.info(\"Menghapus duplikasi berdasarkan konten...\")\n",
        "    df = remove_duplicates(df)\n",
        "    logger.info(f\"Jumlah data setelah menghapus duplikasi: {len(df)}\")\n",
        "\n",
        "    logger.info(\"Memfilter teks berdasarkan bahasa dan relevansi...\")\n",
        "    df = df[df['cleaned_content'].apply(is_valid_text)].reset_index(drop=True)\n",
        "    logger.info(f\"Jumlah data setelah filter: {len(df)}\")\n",
        "\n",
        "    logger.info(\"Melakukan analisis tambahan...\")\n",
        "    df['tahun'] = df['published_date'].dt.year\n",
        "    berita_per_tahun = df['tahun'].value_counts().sort_index()\n",
        "    logger.info(\"Distribusi berita per tahun:\\n\" + berita_per_tahun.to_string())"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Visualisasi dan Penyimpanan Hasil\n",
        "\n",
        "Terakhir, kita buat visualisasi dan simpan file yang sudah bersih ke folder `images` dan `data`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 838
        },
        "id": "KElPBeL0kLs-",
        "outputId": "e4dc4b88-72c5-4827-d090-640bbdec3659"
      },
      "outputs": [],
      "source": [
        "# Cell 11: Visualisasi dan Simpan Hasil\n",
        "if not df.empty:\n",
        "    # Visualisasi Distribusi per Tahun\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    # Menambahkan 'hue' untuk menghindari warning di versi seaborn mendatang\n",
        "    sns.barplot(x=berita_per_tahun.index, y=berita_per_tahun.values, palette='viridis', hue=berita_per_tahun.index, legend=False)\n",
        "    plt.title('Distribusi Berita Gojek per Tahun', fontsize=14)\n",
        "    plt.xlabel('Tahun', fontsize=12)\n",
        "    plt.ylabel('Jumlah Berita', fontsize=12)\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('images/berita_gojek_per_tahun.png') # Simpan ke folder images\n",
        "    plt.show()\n",
        "\n",
        "    # Word Cloud\n",
        "    wordcloud_text = ' '.join(df['cleaned_content'].dropna())\n",
        "    if wordcloud_text:\n",
        "        wordcloud = WordCloud(width=800, height=400, background_color='white', min_font_size=10).generate(wordcloud_text)\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.imshow(wordcloud, interpolation='bilinear')\n",
        "        plt.axis('off')\n",
        "        plt.title('Word Cloud Konten Berita Gojek (Setelah Preprocessing)', fontsize=14)\n",
        "        plt.savefig('images/wordcloud_gojek.png') # Simpan ke folder images\n",
        "        plt.show()\n",
        "\n",
        "    # Simpan hasil ke file CSV dan Excel di folder 'data'\n",
        "    output_filename = 'data/gojek_news_preprocessed'\n",
        "    df.to_csv(f'{output_filename}.csv', index=False)\n",
        "    df.to_excel(f'{output_filename}.xlsx', index=False)\n",
        "    logger.info(f\"Data disimpan ke {output_filename}.csv dan .xlsx\")\n",
        "    logger.info(f\"Jumlah data akhir: {len(df)} (dari awal {initial_count})\")\n",
        "\n",
        "    # Tampilkan sampel\n",
        "    logger.info(\"Sampel 5 baris pertama setelah preprocessing:\")\n",
        "    display(df[['cleaned_title', 'cleaned_content', 'published_date']].head())"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
